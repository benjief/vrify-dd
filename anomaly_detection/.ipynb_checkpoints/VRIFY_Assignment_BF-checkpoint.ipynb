{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac0d530-30e8-4d95-8b96-1d325ceba3c6",
   "metadata": {},
   "source": [
    "# Mini Python/Interpretation Project for Junior Geophysicist Position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317259f-6858-4b3a-959a-10049402ea02",
   "metadata": {},
   "source": [
    "## Quick Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406b99f-7cc7-4ec1-ba0a-a4c57d1dbcc0",
   "metadata": {},
   "source": [
    "Most of my Python scripts were run in a virtual (Conda) environment with all the necessary libraries installed. These scripts also depend on a specific folder structure, so running them directly in this Jupyter Notebook isn’t feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ede2dd-9b31-4f9d-a815-6f530d254f02",
   "metadata": {},
   "source": [
    "## Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390148b0-fb9f-476a-a600-a5ff81d50ab8",
   "metadata": {},
   "source": [
    "The first step was to organize the data using Python. In total, I wrote 9 scripts, each containing multiple functions, to tackle this. Essentially, `main.py` calls individual parsers/processors for the various file types in the project. The order of processing is: shapefiles (.shp), TIFFs (.tif, .tiff), GDB files (.gdb), XYZ files (.xyz), ERS files (.ers), CSVs, and finally other files (like .pdfs). All of these scripts, including reusable functions in `utils.py`, are included here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cad040-0550-4fb8-b884-45761cc97492",
   "metadata": {},
   "source": [
    "**utils.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc6c43d1-2020-405c-bec2-88c0dbe2d378",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_files(file_path, associated_exts, target_folder, total_size_kb):\n",
    "    base_name = os.path.splitext(file_path)[0]\n",
    "\n",
    "    # Iterate over possible associated extensions (including empty extension)\n",
    "    for ext in associated_exts:\n",
    "        associated_file = base_name if ext == '' else base_name + ext\n",
    "        \n",
    "        if os.path.exists(associated_file):\n",
    "            destination_file = os.path.join(target_folder, os.path.basename(associated_file))\n",
    "            # if ext.endswith('.xml'):\n",
    "            #     prettify_xml(associated_file)\n",
    "            shutil.move(associated_file, target_folder)\n",
    "            print(f\"Moved {associated_file} to {destination_file}\")\n",
    "            total_size_kb += os.path.getsize(destination_file) / 1024  # KB\n",
    "            \n",
    "    return total_size_kb\n",
    "\n",
    "\n",
    "def count_folders(processed_folder_path):\n",
    "    \"\"\"\n",
    "    Counts the number of distinct folders created inside the 'processed' folder.\n",
    "    \n",
    "    Args:\n",
    "    processed_folder_path (str): The path to the 'processed' folder, relative or absolute. \n",
    "                                 Default is 'processed', assumed to be one level below the script's root.\n",
    "\n",
    "    Returns:\n",
    "    int: The total count of distinct folders created inside the 'processed' folder.\n",
    "    \"\"\"\n",
    "    # Check if the processed folder exists\n",
    "    if not os.path.exists(processed_folder_path):\n",
    "        print(f\"The folder '{processed_folder_path}' does not exist.\")\n",
    "        return 0\n",
    "\n",
    "    # Count the number of directories inside the processed folder\n",
    "    folder_count = sum(os.path.isdir(os.path.join(processed_folder_path, name)) for name in os.listdir(processed_folder_path))\n",
    "\n",
    "    return folder_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb1213-c338-473d-9ec6-f9eb5c2e5177",
   "metadata": {},
   "source": [
    "**main.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2dda2ba-d28a-4c1e-be4f-fc67a1d4df7a",
   "metadata": {},
   "source": [
    "import os\n",
    "from process_shapefile import process_shapefile\n",
    "from process_tiff import process_tiff\n",
    "from process_gdb import process_gdb\n",
    "from process_xyz import process_xyz\n",
    "from process_ers import process_ers\n",
    "from process_csv import process_csv\n",
    "from process_other import process_other\n",
    "from utils import count_folders\n",
    "\n",
    "# Define root folder and processed folders\n",
    "root_folder = '../data'\n",
    "processed_folder = os.path.join(root_folder, 'processed')\n",
    "vector_data_folder = os.path.join(processed_folder, 'vector_data')\n",
    "raster_data_folder = os.path.join(processed_folder, 'raster_data')\n",
    "other_data_folder = os.path.join(processed_folder, 'other_data')\n",
    "empty_data_folder = os.path.join(processed_folder, 'empty_data')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "os.makedirs(vector_data_folder, exist_ok=True)\n",
    "os.makedirs(raster_data_folder, exist_ok=True)\n",
    "os.makedirs(other_data_folder, exist_ok=True)\n",
    "os.makedirs(empty_data_folder, exist_ok=True)\n",
    "\n",
    "# Initialize total counts\n",
    "total_shapefiles = 0\n",
    "total_tiffs = 0\n",
    "total_gdb = 0\n",
    "total_xyz = 0\n",
    "total_ers = 0\n",
    "total_csv = 0\n",
    "total_pdf = 0\n",
    "total_other = 0\n",
    "new_folders = 1 # Accounts for the \"processed\" folder\n",
    "\n",
    "# Initialize skipped counts and total size\n",
    "total_skipped = 0\n",
    "total_size_kb = 0\n",
    "\n",
    "# Run processing functions and accumulate results\n",
    "total_shapefiles, total_skipped, total_size_kb = process_shapefile(root_folder, processed_folder, empty_data_folder, vector_data_folder, total_shapefiles, total_skipped, total_size_kb)\n",
    "total_tiffs, total_size_kb = process_tiff(root_folder, processed_folder, raster_data_folder, total_tiffs, total_size_kb)\n",
    "total_gdb, total_size_kb = process_gdb(root_folder, processed_folder, vector_data_folder, total_gdb, total_size_kb)\n",
    "total_xyz, total_skipped, total_size_kb = process_xyz(root_folder, processed_folder, empty_data_folder, vector_data_folder, total_xyz, total_skipped, total_size_kb)\n",
    "total_ers, total_skipped, total_size_kb = process_ers(root_folder, processed_folder, empty_data_folder, raster_data_folder, total_ers, total_skipped, total_size_kb)\n",
    "total_csv, total_skipped, total_size_kb = process_csv(root_folder, processed_folder, empty_data_folder, vector_data_folder, total_csv, total_skipped, total_size_kb)\n",
    "total_other, total_size_kb = process_other(root_folder, processed_folder, other_data_folder, total_other, total_size_kb)\n",
    "\n",
    "# Summary data\n",
    "summary_data = {\n",
    "    \"Total shapefiles processed\": total_shapefiles,\n",
    "    \"Total TIFF files processed\": total_tiffs,\n",
    "    \"Total GDB files processed\": total_gdb,\n",
    "    \"Total XYZ files processed\": total_xyz,\n",
    "    \"Total ERS files processed\": total_ers,\n",
    "    \"Total CSV files processed\": total_csv,\n",
    "    \"Total other files processed\": total_other,\n",
    "    \"Total size of processed files (kB)\": f\"{total_size_kb:.2f}\",\n",
    "    \"New folders created\": f\"{count_folders(processed_folder)}\",\n",
    "    \"Total files skipped (empty)\": total_skipped\n",
    "}\n",
    "\n",
    "# Write the summary file\n",
    "summary_file_path = os.path.join(processed_folder, 'summary.txt')\n",
    "with open(summary_file_path, 'w') as summary_file:\n",
    "    for label, value in summary_data.items():\n",
    "        summary_file.write(f\"{label}: {value}\\n\")\n",
    "\n",
    "print(f\"Summary written to {summary_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d880a-a565-4d59-aeea-be07b446c23f",
   "metadata": {},
   "source": [
    "**process_shapefile.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc93063c-c9ec-443f-9808-c9bafaeea729",
   "metadata": {},
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "from utils import move_files\n",
    "\n",
    "def get_shapefile_crs(shapefile):\n",
    "    crs = shapefile.crs.to_string().replace(\":\", \"_\").replace(\"/\", \"_\") if shapefile.crs else 'CRS_unknown'\n",
    "    return crs\n",
    "\n",
    "def process_shapefile(folder, processed_folder, empty_files_folder, destination_folder, total_files, total_skipped, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.shp'):\n",
    "                total_files += 1\n",
    "                shapefile_path = os.path.join(root, file)\n",
    "                shapefile_size_kb = os.path.getsize(shapefile_path) / 1024  # KB\n",
    "                total_size_kb += shapefile_size_kb\n",
    "\n",
    "                try:\n",
    "                    gdf = gpd.read_file(shapefile_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {shapefile_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if gdf.empty:\n",
    "                    print(\"EMPTY FILE; skipping!\")\n",
    "                    total_skipped += 1\n",
    "                    total_size_kb = move_files(shapefile_path, ['.shp', '.shx', '.dbf', '.prj', '.cpg', '.sbn', '.sbx', '.shp.xml'], empty_files_folder, total_size_kb)\n",
    "                    continue\n",
    "\n",
    "                crs = get_shapefile_crs(gdf)\n",
    "                output_folder = os.path.join(destination_folder, crs)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # csv_file_path = os.path.join(output_folder, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                # gdf.to_csv(csv_file_path, index=False)\n",
    "\n",
    "                total_size_kb = move_files(shapefile_path, ['.shx', '.dbf', '.prj', '.cpg', '.sbn', '.sbx', '.shp.xml'], output_folder, total_size_kb)\n",
    "\n",
    "    # Return the updated totals\n",
    "    return total_files, total_skipped, total_size_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be35a49-2050-4e43-90fa-d63447a86871",
   "metadata": {},
   "source": [
    "**process_tiff.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ef3ab3d-d3e1-4704-a011-5f3f9522fbb8",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "from utils import move_files\n",
    "from osgeo import gdal\n",
    "\n",
    "def get_tiff_crs(tiff_path):\n",
    "    \"\"\"Retrieve the CRS from a TIFF file using GDAL.\"\"\"\n",
    "    dataset = gdal.Open(tiff_path)\n",
    "    if not dataset:\n",
    "        print(f\"Unable to open {tiff_path}\")\n",
    "        return None\n",
    "    proj = dataset.GetProjection()\n",
    "    spatial_ref = gdal.osr.SpatialReference(wkt=proj)\n",
    "    authority = spatial_ref.GetAttrValue(\"AUTHORITY\", 0)\n",
    "    code = spatial_ref.GetAttrValue(\"AUTHORITY\", 1)\n",
    "    return f\"{authority}_{code}\" if authority and code else \"CRS_unknown\"\n",
    "\n",
    "def tiff_to_csv(tiff_path, output_csv_path):\n",
    "    dataset = gdal.Open(tiff_path)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    raster_data = band.ReadAsArray()\n",
    "    geotransform = dataset.GetGeoTransform()\n",
    "    with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Longitude', 'Latitude', 'Value'])\n",
    "        for row in range(raster_data.shape[0]):\n",
    "            for col in range(raster_data.shape[1]):\n",
    "                x = geotransform[0] + col * geotransform[1] + row * geotransform[2]\n",
    "                y = geotransform[3] + col * geotransform[4] + row * geotransform[5]\n",
    "                csv_writer.writerow([x, y, raster_data[row, col]])\n",
    "\n",
    "def process_tiff(folder, processed_folder, destination_folder, total_files, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.tiff') or file.lower().endswith('.tif'):\n",
    "                total_files += 1\n",
    "                tiff_path = os.path.join(root, file)\n",
    "                tiff_size_kb = os.path.getsize(tiff_path) / 1024  # KB\n",
    "                total_size_kb += tiff_size_kb\n",
    "\n",
    "                crs = get_tiff_crs(tiff_path)\n",
    "                crs_folder = crs if crs else \"CRS_unknown\"\n",
    "                output_folder = os.path.join(destination_folder, crs_folder)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                csv_file_path = os.path.join(output_folder, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                tiff_to_csv(tiff_path, csv_file_path)\n",
    "                total_size_kb = move_files(tiff_path, ['.tiff', '.tiff.aux.xml', '.tif', '.tif.xml', '.GRD', '.GRD.gi', '.GRD.xml', '.map', '.map.xml'], output_folder, total_size_kb)\n",
    "\n",
    "    # Return updated totals\n",
    "    return total_files, total_size_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ff3d5-5c55-4c9b-b9ca-d101471841ab",
   "metadata": {},
   "source": [
    "**process_gdb.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d064e785-f554-45b9-89b6-3f0c825b36be",
   "metadata": {},
   "source": [
    "import os\n",
    "from utils import move_files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Extract CRS from .gdb.xml files\n",
    "def extract_crs_from_xml(xml_file):\n",
    "    \"\"\"Extract the CRS (EPSG code) from a .gdb.xml file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Dynamically extract all namespaces from the XML file\n",
    "        namespaces = extract_namespaces(xml_file)\n",
    "\n",
    "        # Find the CRS information (EPSG code)\n",
    "        projection = root.find('.//{*}projection', namespaces)\n",
    "        if projection is not None:\n",
    "            epsg_code = projection.attrib.get('wellknown_epsg')\n",
    "            if epsg_code:\n",
    "                return f\"EPSG_{epsg_code}\"\n",
    "        \n",
    "        return \"CRS_unknown\"\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing {xml_file}: {e}\")\n",
    "        return \"CRS_unknown\"\n",
    "\n",
    "def extract_namespaces(xml_file):\n",
    "    \"\"\"Extract all namespaces from the XML file dynamically.\"\"\"\n",
    "    events = \"start\", \"start-ns\"\n",
    "    namespaces = {}\n",
    "    for event, elem in ET.iterparse(xml_file, events):\n",
    "        if event == \"start-ns\":\n",
    "            prefix, uri = elem\n",
    "            namespaces[prefix] = uri\n",
    "    return namespaces\n",
    "\n",
    "# Process gdb files\n",
    "def process_gdb(folder, processed_folder, destination_folder, total_files, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        # Skip files and directories inside of processed_folder\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.gdb.xml'):\n",
    "                total_files += 1\n",
    "                \n",
    "                xml_file = os.path.join(root, file)\n",
    "\n",
    "                # Extract CRS from the .gdb.xml file\n",
    "                crs = extract_crs_from_xml(xml_file)\n",
    "                crs_folder = crs if crs else \"CRS_unknown\"\n",
    "\n",
    "                # Create folder based on the extracted CRS\n",
    "                output_folder = os.path.join(destination_folder, crs_folder)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # Move the .gdb, .gdb.xml, and .csv files with the same prefix\n",
    "                base_name = os.path.splitext(file)[0]  # Get the prefix without extension\n",
    "                total_size_kb = move_files(os.path.join(root, base_name), ['.gdb', '.gdb.xml', '.csv'], output_folder, total_size_kb) \n",
    "                \n",
    "    # Return updated totals\n",
    "    return total_files, total_size_kb               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b6bcd-6516-4cf5-b4cd-d885a43a4b5d",
   "metadata": {},
   "source": [
    "**process_xyz.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4210b87f-0dd6-4aa6-916b-b6a1fffd4823",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "from utils import move_files\n",
    "\n",
    "# Magnetic data headers (from metadata)\n",
    "magnetic_headers = [\n",
    "    'Flight line number', 'Fiducial number', 'Time (hhmmss)', 'Julian day', 'Year',\n",
    "    'Latitude', 'Longitude', 'Radar Altimeter (meters)', 'Total magnetic field value (nanoTeslas)',\n",
    "    'Residual magnetic field value (nanoTeslas)', 'Diurnal (nanoTeslas)', 'Geology (coded)', \n",
    "    'Residual magnetic field (comprehensive model CM4)'\n",
    "]\n",
    "\n",
    "# Radiometric data headers (from metadata)\n",
    "radiometric_headers = [\n",
    "    'Flight line number', 'Fiducial number', 'Time (hhmmss)', 'Julian day', 'Year', 'Latitude',\n",
    "    'Longitude', 'Radar Altimeter (meters)', 'Residual magnetic field value (nanoTeslas)',\n",
    "    'Geology (coded)', 'Quality flag', 'Apparent Potassium (%)', 'Apparent Uranium (ppm eU)',\n",
    "    'Apparent Thorium (ppm eTh)', 'Uranium-Thorium ratio', 'Uranium-Potassium ratio',\n",
    "    'Thorium-Potassium ratio', 'Total count (counts/second)', 'Atmospheric Bismuth 214 (counts/second)',\n",
    "    'Air temperature (°C)', 'Air pressure (mmHg)'\n",
    "]\n",
    "\n",
    "# Convert XYZ to CSV with appropriate headers (based on metadata)\n",
    "def xyz_to_csv(xyz_path, output_csv_path, headers):\n",
    "    try:\n",
    "        with open(xyz_path, 'r') as xyz_file, open(output_csv_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write headers to CSV\n",
    "            csv_writer.writerow(headers)\n",
    "\n",
    "            # Write the XYZ data to CSV\n",
    "            for line in xyz_file:\n",
    "                # Strip any extra whitespace and split the line on whitespace (handling multiple spaces)\n",
    "                row = line.strip().split()\n",
    "\n",
    "                # Ensure valid rows (skip empty lines)\n",
    "                if len(row) >= len(headers):\n",
    "                    # Write row data (matching the number of headers)\n",
    "                    csv_writer.writerow(row[:len(headers)])\n",
    "\n",
    "        print(f\"XYZ file {xyz_path} converted to {output_csv_path}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert {xyz_path} to CSV: {e}\")\n",
    "\n",
    "# Check if file is empty\n",
    "def is_xyz_file_empty(xyz_file):\n",
    "    try:\n",
    "        with open(xyz_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():  # Check if there's a non-empty line\n",
    "                    return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {xyz_file}: {e}\")\n",
    "        return True\n",
    "\n",
    "    return True  # File is empty if no non-empty lines are found\n",
    "\n",
    "# Process XYZ files\n",
    "def process_xyz(folder, processed_folder, empty_files_folder, destination_folder, total_files, total_skipped, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.xyz'):\n",
    "                total_files += 1\n",
    "                xyz_path = os.path.join(root, file)\n",
    "                xyz_size_kb = os.path.getsize(xyz_path) / 1024  # KB\n",
    "                total_size_kb += xyz_size_kb\n",
    "                \n",
    "                if is_xyz_file_empty(xyz_path):\n",
    "                    total_skipped += 1\n",
    "                    total_size_kb = move_files(xyz_path, ['.xyz'], empty_files_folder, total_size_kb)\n",
    "                    continue\n",
    "                \n",
    "                # Determine which headers to use based on the filename\n",
    "                if 'mag' in file.lower():\n",
    "                    headers = magnetic_headers\n",
    "                elif 'rad' in file.lower():\n",
    "                    headers = radiometric_headers\n",
    "                else:\n",
    "                    print(f\"Unknown file type for {file}, skipping.\")\n",
    "                    continue  # Skip if the file doesn't match known types\n",
    "\n",
    "                # CRS for the XYZ file (NAD27 assumed from metadata)\n",
    "                crs_folder = \"EPSG_4267\"\n",
    "\n",
    "                # Create folder based on the extracted CRS\n",
    "                output_folder = os.path.join(destination_folder, crs_folder)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # Convert XYZ to CSV and move the original XYZ and CSV files\n",
    "                csv_file_path = os.path.join(output_folder, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                xyz_to_csv(xyz_path, csv_file_path, headers)\n",
    "                total_size_kb = move_files(xyz_path, ['.xyz'], output_folder, total_size_kb)\n",
    "                \n",
    "                # Split file name to account for other files associated with XYZ files (JPG and TXT)\n",
    "                xyz_path_without_data_suffix = xyz_path.rsplit('_', 1)[0]\n",
    "                total_size_kb = move_files(xyz_path_without_data_suffix, ['.jpg'], output_folder, total_size_kb)\n",
    "                total_size_kb = move_files(f\"{xyz_path_without_data_suffix}_meta\", ['.txt'], output_folder, total_size_kb)\n",
    "                \n",
    "    # Return updated totals\n",
    "    return total_files, total_skipped, total_size_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b450c1e-9b49-4a56-8115-f4264742c763",
   "metadata": {},
   "source": [
    "**process_ers.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa7e2cc-84e3-4a77-a4ba-0a0109e813b4",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "from utils import move_files\n",
    "from osgeo import gdal, osr\n",
    "\n",
    "# Extract CRS from ERS file\n",
    "def get_ers_crs(ers_file):\n",
    "    \"\"\"Retrieve the CRS from an ERS file using GDAL.\"\"\"\n",
    "    dataset = gdal.Open(ers_file)\n",
    "    if not dataset:\n",
    "        print(f\"Unable to open {ers_file}\")\n",
    "        return None\n",
    "\n",
    "    # Get the spatial reference (CRS) from the ERS file\n",
    "    proj = dataset.GetProjection()\n",
    "    if not proj:\n",
    "        print(f\"No CRS information found in {ers_file}\")\n",
    "        return None\n",
    "\n",
    "    # Use the spatial reference system to parse the projection\n",
    "    spatial_ref = osr.SpatialReference(wkt=proj)\n",
    "\n",
    "    # Extract the authority and code (e.g., EPSG, IAU, etc.)\n",
    "    authority = spatial_ref.GetAttrValue(\"AUTHORITY\", 0)\n",
    "    code = spatial_ref.GetAttrValue(\"AUTHORITY\", 1)\n",
    "\n",
    "    # Handle the case where authority or code is missing\n",
    "    if authority and code:\n",
    "        return f\"{authority}_{code}\"\n",
    "    else:\n",
    "        return \"CRS_unknown\"\n",
    "\n",
    "# Convert ERS to CSV\n",
    "def ers_to_csv(ers_path, output_csv_path):\n",
    "    \"\"\"Convert an ERS file to CSV format.\"\"\"\n",
    "    try:\n",
    "        dataset = gdal.Open(ers_path)\n",
    "        if not dataset:\n",
    "            print(f\"Unable to open {ers_path}\")\n",
    "            return None\n",
    "\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        raster_data = band.ReadAsArray()\n",
    "        geotransform = dataset.GetGeoTransform()\n",
    "\n",
    "        # Write raster data to CSV\n",
    "        with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['Longitude', 'Latitude', 'Value'])\n",
    "            for row in range(raster_data.shape[0]):\n",
    "                for col in range(raster_data.shape[1]):\n",
    "                    x = geotransform[0] + col * geotransform[1] + row * geotransform[2]\n",
    "                    y = geotransform[3] + col * geotransform[4] + row * geotransform[5]\n",
    "                    value = raster_data[row, col]\n",
    "                    csv_writer.writerow([x, y, value])\n",
    "\n",
    "        print(f\"ERS file {ers_path} converted to {output_csv_path}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert {ers_path} to CSV: {e}\")\n",
    "\n",
    "# Check if file is empty\n",
    "def is_ers_file_empty(ers_path):\n",
    "    \"\"\"Check if the ERS file is empty by inspecting its raster bands.\"\"\"\n",
    "    dataset = gdal.Open(ers_path)\n",
    "    if not dataset:\n",
    "        print(f\"Unable to open {ers_path}\")\n",
    "        return True  # Consider it empty if it cannot be opened\n",
    "\n",
    "    band = dataset.GetRasterBand(1)  # Get the first band\n",
    "    if band is None:\n",
    "        return True  # No raster data in the band\n",
    "    \n",
    "    raster_data = band.ReadAsArray()\n",
    "    if raster_data is None or raster_data.size == 0:  # No data in the array\n",
    "        return True\n",
    "\n",
    "    return False  # File is not empty\n",
    "        \n",
    "# Process ERS files\n",
    "def process_ers(folder, processed_folder, empty_files_folder, destination_folder, total_files, total_skipped, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.ers'):\n",
    "                total_files += 1\n",
    "                ers_path = os.path.join(root, file)\n",
    "                ers_size_kb = os.path.getsize(ers_path) / 1024  # KB\n",
    "                total_size_kb += ers_size_kb\n",
    "                \n",
    "                if is_ers_file_empty(ers_path):\n",
    "                    total_skipped += 1\n",
    "                    total_size_kb = move_files(ers_path, ['.ERS', '', '.ERS.gi', '.ERS.aux.xml', '.ERS.xml', '.map', '.map.xml'], empty_files_folder, total_size_kb)\n",
    "                    continue\n",
    "\n",
    "                # Get CRS for the ERS file\n",
    "                crs = get_ers_crs(ers_path)\n",
    "                crs_folder = crs if crs else \"CRS_unknown\"\n",
    "\n",
    "                # Create folder based on the extracted CRS\n",
    "                output_folder = os.path.join(destination_folder, crs_folder)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # Convert ERS to CSV and move the original ERS and CSV files\n",
    "                csv_file_path = os.path.join(output_folder, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                ers_to_csv(ers_path, csv_file_path)\n",
    "                total_size_kb = move_files(ers_path, ['.ERS', '', '.ERS.gi', '.ERS.aux.xml', '.ERS.xml'], output_folder, total_size_kb)\n",
    "        \n",
    "    # Return updated totals\n",
    "    return total_files, total_skipped, total_size_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cd7d4-be3a-4658-8f97-a5cb223acff0",
   "metadata": {},
   "source": [
    "**process_csv.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "969d46f2-ee9b-46a7-9eb9-d35ef6f7e043",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "from utils import move_files\n",
    "\n",
    "def is_csv_empty(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return not any(reader)  # Returns True if there are no rows\n",
    "    \n",
    "\n",
    "def process_csv(folder, processed_folder, empty_files_folder, destination_folder, total_files, total_skipped, total_size_kb, new_folders):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.csv'):\n",
    "                total_files += 1\n",
    "                csv_path = os.path.join(root, file)\n",
    "                csv_size_kb = os.path.getsize(csv_path) / 1024  # KB\n",
    "                total_size_kb += csv_size_kb\n",
    "                \n",
    "                if is_csv_empty(csv_path):\n",
    "                    total_skipped += 1\n",
    "                    total_size_kb = move_files(csv_path, ['.csv'], empty_files_folder, total_size_kb)\n",
    "\n",
    "                total_size_kb = move_files(csv_path, ['.csv'], destination_folder, total_size_kb, new_folders)\n",
    "                    \n",
    "    # Return the updated totals\n",
    "    return total_files, total_skipped, total_size_kb, new_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63560ae-61a6-4812-9174-6813c69c9763",
   "metadata": {},
   "source": [
    "**process_other.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44e1cc23-8bd3-471a-9c30-9679059ed51d",
   "metadata": {},
   "source": [
    "import os\n",
    "from utils import move_files\n",
    "\n",
    "def process_other(folder, processed_folder, destination_folder, total_files, total_size_kb):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        if processed_folder in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_size_kb = os.path.getsize(file_path) / 1024  # KB\n",
    "            total_size_kb += file_size_kb\n",
    "        \n",
    "            total_size_kb = move_files(file_path, ['.pdf', '.xslt', '.GeosoftMeta'], destination_folder, total_size_kb) # TODO: Add to this list if other files remain\n",
    "                    \n",
    "    # Return the updated totals\n",
    "    return total_files, total_size_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50073fcb-7891-4049-9d05-5b612dc8a523",
   "metadata": {},
   "source": [
    "On my system, the script was run in a root directory named \"data\". Once the main script finishes, the files are moved and reorganized into a new file structure (shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfce8a-7afe-4705-9dc9-2d1ec79a2e6c",
   "metadata": {},
   "source": [
    "```\n",
    "    data/\n",
    "    ├── processed/\n",
    "        ├── empty_data\n",
    "        └── other_data\n",
    "        └── raster_data/\n",
    "            ├── EPSG_4326\n",
    "        └── vector_data/\n",
    "            ├── EPSG_4267\n",
    "            └── EPSG_4269\n",
    "            └── EPSG_4326\n",
    "            └── EPSG_26711\n",
    "            └── EPSG_26911\n",
    "        └── summary.txt\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885324ab-0424-4a10-a125-e829d816c031",
   "metadata": {},
   "source": [
    "All vector and raster datasets have their Coordinate Reference Systems (CRS) extracted and are grouped into subfolders by CRS. This helped me quickly understand the different CRSs I was dealing with. Empty files are sent to an \"empty_data\" folder. TIFFs, ERD, GRD, and MAP files are moved to a \"raster_data\" folder, while shapefiles, GDBs, and XYZ files go into \"vector_data\". Non-spatial files (like PDFs) are stored in \"other_data\". The Geosoft formats (GDBs, GRDs, MAPs) were a bit tricky, and I manually converted them to CSVs or GeoTIFFs using Geosoft Viewer. While the script creates new files during this process, nothing is deleted. CSV conversions make Python processing easier, and these files are stored in their respective directories, along with any associated files from the root directory. The XYZ file processing script is a bit specific to the dataset I had, and while it works, a more flexible version would be better for general use. A summary text file is generated at the end, tracking file types, their total size, empty files found, and the number of folders created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e327e1f-9c58-4d88-abc7-32dd7d60a0da",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fce05a-7dda-4180-baf6-d522381ae3ae",
   "metadata": {},
   "source": [
    "After organizing the data, I wrote a script to convert everything to EPSG:4326 (WGS84). I haven’t dealt with CRS conversions in a while, so I ignored any potential side effects (probably not best practice), but the reprojected datasets lined up well in QGIS. The `crs_conversion` script looks through all non-EPSG_4326 subdirectories in the \"vector_data\" folder and converts shapefiles and CSVs to EPSG:4326. The originals are left untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbdd83-4321-44a1-afca-314237386d04",
   "metadata": {},
   "source": [
    "**crs_conversion.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee0875ed-446e-4a18-8a70-ffcbc68b7d07",
   "metadata": {},
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = \"../data/processed/vector_data\"\n",
    "output_dir = os.path.join(root_dir, \"EPSG_4326\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def convert_shapefiles_to_4326(shapefile_path, output_dir):\n",
    "    \"\"\"Convert shapefiles to EPSG:4326 and save them to the output directory.\"\"\"\n",
    "    try:\n",
    "        # Load the shapefile\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "        # Convert CRS to EPSG:4326\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "        # Save the converted shapefile in the output directory\n",
    "        output_shapefile = os.path.join(output_dir, os.path.basename(shapefile_path))\n",
    "        gdf.to_file(output_shapefile)\n",
    "        print(f\"Converted {shapefile_path} to EPSG:4326.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {shapefile_path}: {e}\")\n",
    "\n",
    "def convert_csv_to_4326(csv_path, output_dir):\n",
    "    \"\"\"Convert CSV files with latitude/longitude or X/Y columns to EPSG:4326.\"\"\"\n",
    "    try:\n",
    "        # Check if the CSV file is empty\n",
    "        if os.stat(csv_path).st_size == 0:\n",
    "            print(f\"Skipped empty file: {csv_path}\")\n",
    "            return\n",
    "\n",
    "        # Try reading the CSV with UTF-8 encoding first, and fallback to ISO-8859-1 if it fails\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UTF-8 decoding failed for {csv_path}, trying ISO-8859-1.\")\n",
    "            df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
    "\n",
    "        # Convert column names to lowercase for case-insensitive comparison\n",
    "        columns_lower = [col.lower() for col in df.columns]\n",
    "\n",
    "        # Check for columns (Latitude, Longitude) or (X, Y)\n",
    "        if 'latitude' in columns_lower and 'longitude' in columns_lower:\n",
    "            gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Longitude'], df['Latitude']))\n",
    "        elif 'x' in columns_lower and 'y' in columns_lower:\n",
    "            gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['X'], df['Y']))\n",
    "        else:\n",
    "            print(f\"Skipped file with no coordinates: {csv_path}\")\n",
    "            return\n",
    "        \n",
    "        # Set the original CRS if known, e.g., EPSG:4267 or EPSG:26711. Adjust based on your knowledge of the input CRS.\n",
    "        gdf.set_crs(epsg=4267, inplace=True)  # Assuming input is NAD27, adjust as needed\n",
    "        \n",
    "        # Convert to EPSG:4326\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "        \n",
    "        # Extract the updated coordinates and save back as CSV\n",
    "        df['Longitude'], df['Latitude'] = gdf.geometry.x, gdf.geometry.y\n",
    "        output_csv = os.path.join(output_dir, os.path.basename(csv_path))\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Converted {csv_path} to EPSG:4326.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {csv_path}: {e}\")\n",
    "\n",
    "\n",
    "# Traverse the root directory and process subdirectories\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    # Skip the EPSG_4326 directory\n",
    "    if \"EPSG_4326\" in subdir:\n",
    "        continue\n",
    "    \n",
    "    # Process files in the subdirectory\n",
    "    for file in files:\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        \n",
    "        # Check if it's a shapefile\n",
    "        if file.lower().endswith('.shp'):\n",
    "            convert_shapefiles_to_4326(file_path, output_dir)\n",
    "        \n",
    "        # Check if it's a CSV file\n",
    "        elif file.lower().endswith('.csv'):\n",
    "            convert_csv_to_4326(file_path, output_dir)\n",
    "\n",
    "print(\"CRS conversion completed for all files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bb2c3-1808-46a2-8eec-8edb2aad8412",
   "metadata": {},
   "source": [
    "I wrote `generate_file_overview.py` to help quickly open shapefiles and CSVs, visualize column histograms, and generate key statistics. This helped spot outliers quickly. For instance, some datasets had undefined values set as -999999 or -9999.9, which were immediately obvious in the histograms. I also wrote `clean_data.py`, which allows users to set upper and lower bounds for data cleaning. It currently only handles numeric data, but expanding it to handle non-numeric data would be straightforward. The scripts, a supporting `util.py`, and a few histograms (pre- and post-cleaning) are included below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e7096-1cfe-449d-9620-69143cf215f2",
   "metadata": {},
   "source": [
    "**util.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a25b13f6-e91c-4f08-a63c-6bb96324452a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, simpledialog\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def select_file(filetype=None):\n",
    "    \"\"\"\n",
    "    Open a file dialog to allow the user to select a file based on specified file types.\n",
    "    \n",
    "    Parameters:\n",
    "    filetype (list of tuples): Optional parameter to specify file types. \n",
    "                               Format: [(\"Description\", \"*.extension\"), ...].\n",
    "                               If None, defaults to CSV and Shapefiles.\n",
    "    \n",
    "    Returns:\n",
    "    str: The file path of the selected file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "\n",
    "    # Default to CSV and Shapefiles if no filetype is provided\n",
    "    if filetype is None:\n",
    "        filetype = [(\"CSV and Shapefiles\", \"*.csv;*.shp\")]\n",
    "    \n",
    "    # Open file dialog to select a file based on the provided or default file type\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a File\", \n",
    "        filetypes=filetype)  # Limit selection to specified file types\n",
    "\n",
    "    if file_path:\n",
    "        print(f\"Selected file: {file_path}\")\n",
    "    else:\n",
    "        print(\"No file selected.\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    \"\"\"Load the selected file into a DataFrame or GeoDataFrame.\"\"\"\n",
    "    if file_path.lower().endswith('.csv'):\n",
    "        # Try reading the CSV with UTF-8 encoding first, and fallback to ISO-8859-1 if it fails\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UTF-8 decoding failed for {file_path}, trying ISO-8859-1.\")\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    elif file_path.lower().endswith('.shp'):\n",
    "        # Load Shapefile into geopandas GeoDataFrame\n",
    "        df = gpd.read_file(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_path}\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "def select_column(df, prompt):\n",
    "    \"\"\"Prompt the user to select a column.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # List the available columns vertically\n",
    "    available_columns = '\\n'.join(df.columns.tolist())  # Join columns with newlines\n",
    "    column_name = simpledialog.askstring(\"Input\", f\"Available columns:\\n\\n{available_columns}\\n\\n{prompt}\")\n",
    "    \n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    return column_name\n",
    "\n",
    "def select_multiple_columns(df, prompt):\n",
    "    \"\"\"Prompt the user to select columns.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # List the available columns vertically\n",
    "    available_columns = '\\n'.join(df.columns.tolist())  # Join columns with newlines\n",
    "\n",
    "    # Allow user to input columns separated by commas\n",
    "    columns = simpledialog.askstring(\"Input\", f\"{prompt}\\n\\nAvailable columns:\\n\\n{available_columns}\")\n",
    "\n",
    "    # Split and clean the input into a list of columns\n",
    "    selected_columns = [col.strip() for col in columns.split(',') if col.strip() in df.columns]\n",
    "    \n",
    "    if not selected_columns:\n",
    "        raise ValueError(\"No valid columns selected.\")\n",
    "\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef561d70-2576-4cdd-b6a7-4257e4294c71",
   "metadata": {},
   "source": [
    "**generate_file_overview.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "887eeb90-f535-4e27-a822-f6438dceba0d",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import  messagebox\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import select_file, load_file, select_column, select_multiple_columns\n",
    "\n",
    "def plot_histogram(df, column):\n",
    "    \"\"\"Plot a histogram of the selected column.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    df[column].hist(bins=50, ax=ax)\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(column, fontsize=12)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "    \n",
    "    # Rotate the x-axis labels for better visibility\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f\"Histogram of {column}\", fontsize=14)\n",
    "    \n",
    "    # Automatically adjust layout to fit labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def prompt_for_visualization():\n",
    "    \"\"\"Ask the user if they want to visualize data.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    return messagebox.askyesno(\"Data Visualization\", \"Do you want to visualize a column as a frequency histogram?\")\n",
    "\n",
    "def prompt_for_stats():\n",
    "    \"\"\"Ask the user if they want to generate key statistics.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    return messagebox.askyesno(\"Key Statistics\", \"Do you want to generate key statistics for any columns?\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_key_statistics(df, columns):\n",
    "    \"\"\"Generate key statistics (min, max, mean, median, mode, std, var) for selected columns.\"\"\"\n",
    "    for column in columns:\n",
    "        print(f\"\\nStatistics for column: {column}\")\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            print(f\"Min: {df[column].min()}\")\n",
    "            print(f\"Max: {df[column].max()}\")\n",
    "            print(f\"Mean: {df[column].mean()}\")\n",
    "            print(f\"Median: {df[column].median()}\")\n",
    "            print(f\"Mode: {df[column].mode()[0] if not df[column].mode().empty else 'N/A'}\")\n",
    "            print(f\"Standard Deviation: {df[column].std()}\")\n",
    "            print(f\"Variance: {df[column].var()}\")\n",
    "            print(f\"Skewness: {df[column].skew()}\")\n",
    "            print(f\"Kurtosis: {df[column].kurt()}\")\n",
    "        elif pd.api.types.is_object_dtype(df[column]):\n",
    "            print(f\"Mode: {df[column].mode()[0] if not df[column].mode().empty else 'N/A'}\")\n",
    "        else:\n",
    "            print(f\"Column {column} is not numeric or categorical. Skipping...\")\n",
    "\n",
    "# Main execution\n",
    "file_path = select_file()\n",
    "\n",
    "if file_path:\n",
    "    # Load the file into a DataFrame/GeoDataFrame\n",
    "    df = load_file(file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Display the first 5 rows of data\n",
    "        print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "        # Print the column names and their data types\n",
    "        print(df.info())\n",
    "        \n",
    "        # Ask the user if they want to visualize data\n",
    "        if prompt_for_visualization():\n",
    "            try:\n",
    "                # Prompt the user to select the column to visualize\n",
    "                column_to_visualize = select_column(df, \"Which column would you like to visualize as frequency histogram?\")\n",
    "                # Plot a histogram of values for the selected column\n",
    "                plot_histogram(df, column_to_visualize)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "\n",
    "        # Ask the user if they want to generate key statistics\n",
    "        if prompt_for_stats():\n",
    "            # Let the user select columns for which to generate statistics\n",
    "            columns_to_analyze = select_multiple_columns(df, \"Enter columns (separated by commas) for which you want key statistics\")\n",
    "\n",
    "            # Generate and display key statistics for the selected columns\n",
    "            generate_key_statistics(df, columns_to_analyze)\n",
    "else:\n",
    "    print(\"No file selected. Exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b9cc5-54f0-4df8-83e3-5e080b8eaab7",
   "metadata": {},
   "source": [
    "**clean_data.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd74b198-cb61-432f-be6e-1b176c638236",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox, filedialog\n",
    "from utils import select_file, load_file, select_column\n",
    "\n",
    "def get_cleaning_range(column_name):\n",
    "    \"\"\"Prompt the user for the inclusive range of values to keep in the selected column.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Prompt for the lower bound\n",
    "    lower_bound = simpledialog.askstring(\"Input\", f\"Enter the lower bound of the range for the column '{column_name}':\")\n",
    "    if lower_bound is None:\n",
    "        messagebox.showerror(\"Error\", \"No lower bound provided.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Prompt for the upper bound\n",
    "    upper_bound = simpledialog.askstring(\"Input\", f\"Enter the upper bound of the range for the column '{column_name}':\")\n",
    "    if upper_bound is None:\n",
    "        messagebox.showerror(\"Error\", \"No upper bound provided.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Remove leading/trailing spaces and convert to numeric if possible\n",
    "    lower_bound = lower_bound.strip()\n",
    "    upper_bound = upper_bound.strip()\n",
    "\n",
    "    # Convert to numeric values\n",
    "    try:\n",
    "        lower_bound = pd.to_numeric(lower_bound, errors='raise')\n",
    "        upper_bound = pd.to_numeric(upper_bound, errors='raise')\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Error\", \"Invalid input. Please enter numeric values for the range.\")\n",
    "        return None, None\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def clean_data_by_range(df, column, lower_bound, upper_bound):\n",
    "    \"\"\"Remove rows from the DataFrame where the selected column is outside the specified inclusive range.\"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Check if the column is numeric\n",
    "    if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "        messagebox.showerror(\"Error\", f\"Column '{column}' is not numeric.\")\n",
    "        return df\n",
    "    \n",
    "    # Remove rows outside the specified range (inclusive)\n",
    "    df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    final_count = len(df_cleaned)\n",
    "    rows_removed = initial_count - final_count\n",
    "    \n",
    "    print(f\"Rows removed: {rows_removed}\")\n",
    "    return df_cleaned\n",
    "\n",
    "def save_cleaned_file(df, file_path):\n",
    "    \"\"\"Save the cleaned DataFrame or GeoDataFrame to a file.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # Prompt the user to select a save location\n",
    "    save_path = filedialog.asksaveasfilename(\n",
    "        defaultextension=\".csv\" if file_path.endswith(\".csv\") else \".shp\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\"), (\"Shapefiles\", \"*.shp\")],\n",
    "        title=\"Save Cleaned File\"\n",
    "    )\n",
    "    \n",
    "    if save_path:\n",
    "        if file_path.lower().endswith(\".csv\"):\n",
    "            # Save as CSV\n",
    "            df.to_csv(save_path, index=False)\n",
    "        elif file_path.lower().endswith(\".shp\"):\n",
    "            # Save as Shapefile\n",
    "            df.to_file(save_path)\n",
    "        print(f\"File saved as: {save_path}\")\n",
    "    else:\n",
    "        print(\"Save operation cancelled.\")\n",
    "\n",
    "# Main execution\n",
    "file_path = select_file()\n",
    "\n",
    "if file_path:\n",
    "    # Load the file into a DataFrame/GeoDataFrame\n",
    "    df = load_file(file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Prompt the user to select a column\n",
    "        column_to_clean = select_column(df, \"Which column would you like to clean?\")\n",
    "        \n",
    "        # Prompt the user to input a range of values to keep\n",
    "        lower_bound, upper_bound = get_cleaning_range(column_to_clean)\n",
    "        \n",
    "        if lower_bound is not None and upper_bound is not None:\n",
    "            # Clean the data by the specified range\n",
    "            df_cleaned = clean_data_by_range(df, column_to_clean, lower_bound, upper_bound)\n",
    "            \n",
    "            # Save the cleaned file\n",
    "            save_cleaned_file(df_cleaned, file_path)\n",
    "else:\n",
    "    print(\"No file selected. Exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd618c81-43d2-49c1-bd0e-e0d7431b1166",
   "metadata": {},
   "source": [
    "![A histogram showing frequencies of the Apparent Potassium (%) column in the original reno_mag dataset](figures_for_jn/f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd585801-005d-4b79-aa9f-7def15d79d38",
   "metadata": {},
   "source": [
    "**Figure 1**: A histogram showing frequencies of the Apparent Potassium (%) column in the original `reno_mag` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af7cec-b0d2-492c-9b89-daa91654ea4e",
   "metadata": {},
   "source": [
    "![A histogram showing frequencies of the Apparent Potassium (%) column in the cleaned reno_mag dataset](figures_for_jn/f2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfce59-46ad-4131-a518-df3449035002",
   "metadata": {},
   "source": [
    "**Figure 2**: A histogram showing frequencies of the Apparent Potassium (%) column in the _cleaned_ reno_mag dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cbf1a-4831-4ab6-835c-fa93b47fe757",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01a883-db1f-478d-87b0-5334cbf700e7",
   "metadata": {},
   "source": [
    "I attempted to visualize all the data using Python but quickly realized I was in over my head. Rendering multiple data types on a map proved challenging, especially in Folium. The map rendering was slow, layers were messy (see Figure 3), I couldn’t figure out how to add a legend, and nothing was as interactive as I wanted. In the end, I gave up and switched to QGIS, which turned out to be much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2669-a520-4863-818b-7e6d5ebba0c0",
   "metadata": {},
   "source": [
    "**visualize_files.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e7902e3-207a-4a21-9b07-45b2f69b76d6",
   "metadata": {},
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, simpledialog\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def select_files(file_type):\n",
    "    \"\"\"Open a file dialog to select multiple files of a given type.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "\n",
    "    file_paths = filedialog.askopenfilenames(title=f\"Select {file_type} files\", \n",
    "                                             filetypes=[(file_type, f\"*.{file_type}\")])\n",
    "    return list(file_paths)\n",
    "\n",
    "def select_column(df):\n",
    "    \"\"\"Prompt the user to select which column to visualize, excluding Latitude and Longitude.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # Filter out columns related to Latitude and Longitude\n",
    "    excluded_columns = ['latitude', 'longitude', 'lat', 'lon', 'x', 'y']\n",
    "    available_columns = [col for col in df.columns if col.lower() not in excluded_columns]\n",
    "    \n",
    "    # Prompt the user to select which column to visualize\n",
    "    column_name = simpledialog.askstring(\"Input\", f\"Available columns: {', '.join(available_columns)}\\nWhich column would you like to visualize?\")\n",
    "    \n",
    "    if column_name not in available_columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    return column_name\n",
    "\n",
    "def select_shapefile_column(gdf):\n",
    "    \"\"\"Prompt the user to select which column to visualize in the shapefile.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    available_columns = gdf.columns.tolist()  # Get all attribute columns\n",
    "    \n",
    "    # Prompt the user to select which column to visualize\n",
    "    column_name = simpledialog.askstring(\"Input\", f\"Available columns: {', '.join(available_columns)}\\nWhich column would you like to visualize?\")\n",
    "    \n",
    "    if column_name not in available_columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the shapefile.\")\n",
    "    \n",
    "    return column_name\n",
    "\n",
    "def find_lat_lon_columns(df):\n",
    "    \"\"\"Find latitude and longitude (or equivalent) columns in the DataFrame.\"\"\"\n",
    "    possible_lat_columns = ['latitude', 'lat', 'y', 'Y']\n",
    "    possible_lon_columns = ['longitude', 'lon', 'x', 'X']\n",
    "    \n",
    "    lat_column = next((col for col in df.columns if col.lower() in [name.lower() for name in possible_lat_columns]), None)\n",
    "    lon_column = next((col for col in df.columns if col.lower() in [name.lower() for name in possible_lon_columns]), None)\n",
    "    \n",
    "    if lat_column is None or lon_column is None:\n",
    "        raise ValueError(\"Error: Could not find Latitude and Longitude columns in the file.\")\n",
    "    \n",
    "    return lat_column, lon_column\n",
    "\n",
    "def plot_shapefiles(map_obj, shapefiles):\n",
    "    \"\"\"Plot multiple shapefiles on the map without custom styling.\"\"\"\n",
    "    for shapefile in shapefiles:\n",
    "        gdf = gpd.read_file(shapefile)\n",
    "        \n",
    "        folium.GeoJson(\n",
    "            gdf.to_json(), \n",
    "            name=shapefile.split('/')[-1]\n",
    "        ).add_to(map_obj)\n",
    "\n",
    "def plot_lines(map_obj, line_files):\n",
    "    \"\"\"Plot multiple line CSV files on the map, with proper handling of negative values.\"\"\"\n",
    "    for line_file in line_files:\n",
    "        df = pd.read_csv(line_file)\n",
    "        \n",
    "        try:\n",
    "            lat_column, lon_column = find_lat_lon_columns(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"{e} in file {line_file}.\")\n",
    "            continue\n",
    "        \n",
    "        # Ask the user to select which column to visualize\n",
    "        try:\n",
    "            value_column = select_column(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"{e} in file {line_file}.\")\n",
    "            continue\n",
    "        \n",
    "        values = df[value_column]\n",
    "\n",
    "        # Handle negative values by shifting the range if necessary\n",
    "        min_value = values.min()\n",
    "        if min_value < 0:\n",
    "            values += abs(min_value)  # Shift the values so they are all positive for color mapping\n",
    "\n",
    "        # Normalize the selected column values to a range of 0-1 after shift\n",
    "        norm = plt.Normalize(vmin=values.min(), vmax=values.max())\n",
    "\n",
    "        # Use a diverging colormap (e.g., 'coolwarm') for more variability\n",
    "        colormap = plt.get_cmap('coolwarm')\n",
    "\n",
    "        # Plot the line segments, coloring each segment based on the normalized value\n",
    "        for i in range(len(df) - 1):\n",
    "            coords = [(df.iloc[i][lat_column], df.iloc[i][lon_column]), (df.iloc[i + 1][lat_column], df.iloc[i + 1][lon_column])]\n",
    "            value = df.iloc[i][value_column]\n",
    "            \n",
    "            # Map the value to a color using the colormap\n",
    "            color = mcolors.to_hex(colormap(norm(value)))  # Convert the color to hex for folium\n",
    "            \n",
    "            folium.PolyLine(locations=coords, color=color, weight=2.5, opacity=0.8, name=line_file.split('/')[-1]).add_to(map_obj)\n",
    "\n",
    "\n",
    "def plot_heatmaps(map_obj, heatmap_files):\n",
    "    \"\"\"Plot heatmaps with negative values handled appropriately.\"\"\"\n",
    "    for heatmap_file in heatmap_files:\n",
    "        df = pd.read_csv(heatmap_file)\n",
    "        \n",
    "        try:\n",
    "            lat_column, lon_column = find_lat_lon_columns(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"{e} in file {heatmap_file}.\")\n",
    "            continue\n",
    "        \n",
    "        # Ask the user to select the column for heatmap visualization\n",
    "        try:\n",
    "            value_column = select_column(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"{e} in file {heatmap_file}.\")\n",
    "            continue\n",
    "        \n",
    "        values = df[value_column]\n",
    "        \n",
    "        # To handle negative values, shift the data by adding the absolute minimum value to all values if necessary\n",
    "        min_value = values.min()\n",
    "        if min_value < 0:\n",
    "            values += abs(min_value)  # Shift values to ensure all are positive for heatmap generation\n",
    "\n",
    "        # Create heatmap data using the adjusted value column\n",
    "        heat_data = [[row[lat_column], row[lon_column], row[value_column]] for index, row in df.iterrows()]\n",
    "\n",
    "        # Customize the heatmap for a smoother look\n",
    "        heatmap = HeatMap(heat_data, name=heatmap_file.split('/')[-1], \n",
    "                          radius=20, blur=30, max_zoom=1, \n",
    "                          gradient={0.4: 'blue', 0.65: 'lime', 1: 'red'})  # Adjust these values as needed\n",
    "        \n",
    "        map_obj.add_child(heatmap)\n",
    "\n",
    "# Main execution\n",
    "# Create a base map\n",
    "base_map = folium.Map(location=[39.5, -119.5], zoom_start=8)  # Adjust location based on your data\n",
    "\n",
    "# Ask the user to select shapefiles\n",
    "shapefiles = select_files(\"shp\")\n",
    "if shapefiles:\n",
    "    plot_shapefiles(base_map, shapefiles)\n",
    "\n",
    "# Ask the user to select CSV files for line plotting\n",
    "line_files = select_files(\"csv\")\n",
    "if line_files:\n",
    "    plot_lines(base_map, line_files)\n",
    "\n",
    "# Ask the user to select CSV files for heatmaps\n",
    "heatmap_files = select_files(\"csv\")\n",
    "if heatmap_files:\n",
    "    plot_heatmaps(base_map, heatmap_files)\n",
    "\n",
    "# Add a layer control to toggle between different layers\n",
    "folium.LayerControl().add_to(base_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "base_map.save('interactive_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db80d6-2cac-4344-9a58-049967f99bb1",
   "metadata": {},
   "source": [
    "![The \"interactive\" Folium map I created, displaying DNAG magnetic field data, residual magnetic field strength lines and significant deposits](figures_for_jn/f3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70400557-1fc5-49da-8e84-904700d44dc6",
   "metadata": {},
   "source": [
    "**Figure 3**: The \"interactive\" Folium map I created, displaying DNAG magnetic field data, residual magnetic field strength lines and significant deposits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b8ac5-4805-4603-a0e0-88d9507fb0e6",
   "metadata": {},
   "source": [
    "After visualizing everything, I focused on a subset of files that I found intriguing. A map, generated in QGIS, is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72226e-11b0-4552-b114-a3e78a17759a",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing the total magnetic field strength (DNAG), the residual magnetic field strength (reno_mag and water_lake_mag), faults, precious metals, significant deposits, and active mining claims](figures_for_jn/f4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68bab7-2ec4-46c7-9d54-d460aa3acc49",
   "metadata": {},
   "source": [
    "**Figure 4**: A QGIS-generated map showing the total magnetic field strength (DNAG), the residual magnetic field strength (reno_mag and water_lake_mag), faults, precious metals, significant deposits, and active mining claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a49ac7-05b3-4035-8d8b-8d08fe7dc31b",
   "metadata": {},
   "source": [
    "What really caught my attention were the positive residual magnetic field anomalies near the Cu-colored precious metal markers on the map’s eastern side. Magnetic minerals can be linked to hydrothermal systems (e.g., magnetite forms as a byproduct of hydrothermal alteration). This might suggest some association with porphyry copper systems, which could explain why the precious metals markers are in this area. Given my interest in the granularity of the reno and walker_lake XYZ files, I also checked out the radiometric data along these lines, and apparent potassium values were relatively high in this zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9d31c-6a47-4bf0-b891-b81193794637",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing the total magnetic field strength (DNAG), apparent potassium (reno_rad and water_lake_rad), faults, precious metals, significant deposits, and active mining claims](figures_for_jn/f5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65681bb-bc32-4004-a72c-791fe40d406e",
   "metadata": {},
   "source": [
    "**Figure 5**: A QGIS-generated map showing the total magnetic field strength (DNAG), apparent potassium (reno_rad and water_lake_rad), faults, precious metals, significant deposits, and active mining claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873d409-32b1-4665-8dfc-6bcc928109da",
   "metadata": {},
   "source": [
    "I wanted to explore Bouguer anomalies in the area (one of the few things I vaguely remember from school). After digging through the provided files, I found the USA_isograv dataset, which seemed to have what I needed. Because point data isn’t as fun as raster data, I wrote a script to convert point data (from CSVs) into Matplotlib heatmaps and then GeoTIFFs (with Rasterio). However, Rasterio kept rotating the heatmaps 90 degrees. After some trial and error, I fixed this by transposing the z_grid and flipping it. The Bouguer anomaly heatmap, along with the point data, can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4daca5-3ea0-48e5-9a61-8c97c69b6e23",
   "metadata": {},
   "source": [
    "**heatmap_generation_from_csv.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc8d6610-6cb5-415e-aea3-d166405ca444",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from scipy.interpolate import griddata\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, filedialog\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from utils import select_file, select_column\n",
    "\n",
    "def get_heatmap_label():\n",
    "    \"\"\"Prompt the user for x-label, y-label, title, and legend label.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # Prompt for the x-label, y-label, and title\n",
    "    x_label = simpledialog.askstring(\"Input\", \"Enter x-label for the heatmap:\")\n",
    "    y_label = simpledialog.askstring(\"Input\", \"Enter y-label for the heatmap:\")\n",
    "    plot_title = simpledialog.askstring(\"Input\", \"Enter title for the heatmap:\")\n",
    "    legend_label = simpledialog.askstring(\"Input\", \"Enter label for the legend:\")\n",
    "    \n",
    "    return x_label, y_label, plot_title, legend_label   \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from scipy.interpolate import griddata\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from utils import select_file, select_column\n",
    "\n",
    "def get_heatmap_label():\n",
    "    \"\"\"Prompt the user for x-label, y-label, title, and legend label.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # Prompt for the x-label, y-label, and title\n",
    "    x_label = simpledialog.askstring(\"Input\", \"Enter x-label for the heatmap:\")\n",
    "    y_label = simpledialog.askstring(\"Input\", \"Enter y-label for the heatmap:\")\n",
    "    plot_title = simpledialog.askstring(\"Input\", \"Enter title for the heatmap:\")\n",
    "    legend_label = simpledialog.askstring(\"Input\", \"Enter label for the legend:\")\n",
    "    \n",
    "    return x_label, y_label, plot_title, legend_label    \n",
    "\n",
    "def save_as_geotiff(grid_x, grid_y, grid_z, lon_min, lat_max, output_path):\n",
    "    \"\"\"Save the heatmap as a GeoTIFF file.\"\"\"\n",
    "    transform = from_origin(lon_min, lat_max, (grid_x[1, 0] - grid_x[0, 0]), (grid_y[0, 1] - grid_y[0, 0]))\n",
    "    \n",
    "    # Fix issues with transform (the transform introduces a 90-degree clockwise rotation)\n",
    "    grid_z = np.transpose(grid_z)\n",
    "    grid_z = np.flipud(grid_z)  \n",
    "\n",
    "    with rasterio.open(\n",
    "        output_path, 'w', driver='GTiff',\n",
    "        height=grid_z.shape[0], width=grid_z.shape[1],\n",
    "        count=1, dtype=grid_z.dtype, crs='EPSG:4326',\n",
    "        transform=transform) as dst:\n",
    "        dst.write(grid_z, 1)\n",
    "    print(f\"GeoTIFF saved to {output_path}\")\n",
    "    \n",
    "def prompt_for_export(grid_x, grid_y, grid_z):\n",
    "    \"\"\"Ask the user if they want to export the heatmap as GeoTIFF.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    export_format = simpledialog.askstring(\"Export\", \"Would you like to export the heatmap as a GeoTIFF? (Enter 'Yes' or 'No'):\")\n",
    "    if export_format.lower() == 'yes':\n",
    "        file_path = filedialog.asksaveasfilename(defaultextension=\".tif\", filetypes=[(\"GeoTIFF files\", \"*.tif\")])\n",
    "        if file_path:\n",
    "            save_as_geotiff(grid_x, grid_y, grid_z, grid_x.min(), grid_y.max(), file_path)\n",
    "    else:\n",
    "        print(\"No export requested.\")\n",
    "\n",
    "# Get the selected CSV file\n",
    "csv_file_path = select_file([(\"CSV Files\", \"*.csv\")])\n",
    "\n",
    "# Proceed only if a file was selected\n",
    "if csv_file_path:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Prompt the user to select the column to visualize\n",
    "    column_to_visualize = select_column(df, \"Which column would you like to generate a heatmap for?\")\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "    \n",
    "    # Extract longitude, latitude, and the selected column's data\n",
    "    lon = df['Longitude']\n",
    "    lat = df['Latitude']\n",
    "    values = df[column_to_visualize]\n",
    "\n",
    "    # Create a grid to interpolate the values onto\n",
    "    grid_x, grid_y = np.mgrid[lon.min():lon.max():500j, lat.min():lat.max():500j]\n",
    "\n",
    "    # Interpolate the selected column's values onto the grid\n",
    "    grid_z = griddata((lon, lat), values, (grid_x, grid_y), method='cubic')\n",
    "    \n",
    "    # Prompt the user for x-label, y-label, title, and legend label\n",
    "    x_label, y_label, plot_title, legend_label = get_heatmap_label()\n",
    "\n",
    "    # Plot the interpolated heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap = plt.pcolormesh(grid_x, grid_y, grid_z, cmap='hsv', shading='auto')\n",
    "    plt.colorbar(heatmap).set_label(legend_label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()\n",
    "    \n",
    "    # Prompt the user to export the heatmap as GeoTIFF\n",
    "    prompt_for_export(grid_x, grid_y, grid_z)\n",
    "else:\n",
    "    print(\"No CSV file was selected. Exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46466cfe-508f-4f96-b171-cc25a474fe4b",
   "metadata": {},
   "source": [
    "![A Matplotlib heatmap for isostatically-corrected Bouguer gravity anomalies](figures_for_jn/f6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a6193-750d-44b2-98a2-0fa3d18c8ee9",
   "metadata": {},
   "source": [
    "**Figure 6**: A Matplotlib heatmap for isostatically-corrected Bouguer gravity anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9f0f0-a38d-4c6e-becc-799bd9779d86",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing the same heatmap (after being converted to a GeoTIFF), faults, precious metals, significant deposits, and active mining claims](figures_for_jn/f7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1edd03-5390-429b-ab9a-0b1f86bebffd",
   "metadata": {},
   "source": [
    "**Figure 7**: A QGIS-generated map showing the same heatmap (after being converted to a GeoTIFF), faults, precious metals, significant deposits, and active mining claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f35b5a-7008-4840-b457-c6b534af4e90",
   "metadata": {},
   "source": [
    "To my untrained eye, this didn't provide much insight with respect to my hydrothermal alteration hypothesis. However, all of the known deposits on the map appear to be associated with similar, but relatively high Bouguer anomalies. Perhaps this is something that could be taken into consideration in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989b62e-38a6-4039-af05-4d3ba34ef504",
   "metadata": {},
   "source": [
    "## Advanced Analysis (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121091b-7cf0-4312-9bb7-20d4e6946c67",
   "metadata": {},
   "source": [
    "Excited about the data, I decided to perform K-Means clustering on the combined reno_rad and walker_lake_rad datasets (combined as CSVs). These datasets contained granular info on residual magnetic fields and radiometric data, which I found potentially interesting for exploration. K-Means clustering is easy to use, computationally efficient, and relatively simple to interpret, but it has its drawbacks, especially for geological data. Picking the wrong k can hide key features, and the algorithm assumes clusters are spherical, which doesn’t reflect real-world geology. To help find a good k, I used the Elbow Method, which plots the sum of squared distances between points and their cluster centroids. I settled on k = 5, clustering based on residual magnetic fields, apparent thorium, and apparent potassium values. Below is the script, the elbow plot, and cluster plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864c9e8-8205-47d4-afb1-35ed92269254",
   "metadata": {},
   "source": [
    "**k_means_clustering.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77850c39-ca6b-4ee0-8d20-e8e6b37aa9e2",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_filter_clean_data():\n",
    "    # Load rad datasets \n",
    "    df1 = pd.read_csv('../data/processed/vector_data/EPSG_4326/reno_rad_cleaned.csv')  \n",
    "    df2 = pd.read_csv('../data/processed/vector_data/EPSG_4326/walker_lake_rad_cleaned.csv')  \n",
    "\n",
    "    # Concatenate datasets into one larger DataFrame\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Select the columns for clustering\n",
    "    features = ['Residual magnetic field value (nanoTeslas)', 'Apparent Thorium (ppm eTh)', 'Apparent Potassium (%)']\n",
    "\n",
    "    # Filter the DataFrame to keep only the selected features\n",
    "    df_filtered = df[features]\n",
    "\n",
    "    # Handle missing values if there are any\n",
    "    df_filtered = df_filtered.dropna()\n",
    "\n",
    "    # Normalize the selected features (standard scaling)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_filtered)\n",
    "\n",
    "    return df, scaled_features\n",
    "\n",
    "def get_ideal_num_clusters(scaled_features):\n",
    "    # Calculate inertia for a range of cluster numbers\n",
    "    inertia = []\n",
    "    k_values = range(1, 20)\n",
    "\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(scaled_features)\n",
    "        inertia.append(kmeans.inertia_)  # Inertia is the sum of squared distances to the nearest centroid\n",
    "\n",
    "    # Plot the inertia to find the elbow point\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k_values, inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia (Sum of squared distances)')\n",
    "    plt.show()\n",
    "    \n",
    "def perform_clustering(df, scaled_features, n_clusters):\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters, random_state=42) \n",
    "    kmeans.fit(scaled_features)\n",
    "\n",
    "    # Add the cluster labels back to the original DataFrame\n",
    "    df['Cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Save the DataFrame with clusters as a CSV file\n",
    "    df.to_csv(\"../data/processed/vector_data/EPSG_4326/reno_walker_lake_combined_km_clustered.csv\", index=False)\n",
    "    \n",
    "    return df, scaled_features\n",
    "\n",
    "def visualize_clusters(df):\n",
    "    # Visualize the clusters with Residual Magnetic Field and Thorium\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=df, x='Residual magnetic field value (nanoTeslas)', y='Apparent Thorium (ppm eTh)', hue='Cluster', palette='viridis')\n",
    "    plt.title('K-Means Clustering based on Magnetic Field and Thorium')\n",
    "    plt.xlabel('Residual Magnetic Field (nT)')\n",
    "    plt.ylabel('Apparent Thorium (ppm eTh)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the clusters with Residual Magnetic Field and Potassium\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=df, x='Residual magnetic field value (nanoTeslas)', y='Apparent Potassium (%)', hue='Cluster', palette='viridis')\n",
    "    plt.title('K-Means Clustering based on Magnetic Field and Potassium')\n",
    "    plt.xlabel('Residual Magnetic Field (nT)')\n",
    "    plt.ylabel('Apparent Potassium (%)')\n",
    "    plt.show()\n",
    "\n",
    "df, scaled_features = load_filter_clean_data()\n",
    "get_ideal_num_clusters(scaled_features)\n",
    "n_clusters = 5\n",
    "perform_clustering(df, scaled_features, n_clusters)\n",
    "visualize_clusters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec35fa-38f1-4e92-a930-bbd883a82c96",
   "metadata": {},
   "source": [
    "![A plot generated by the Elbow Method, used to decide on a value for k](figures_for_jn/f8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5054a-e55b-40f3-afa7-22c0e531c042",
   "metadata": {},
   "source": [
    "**Figure 8**: A plot generated by the Elbow Method, used to decide on a value for k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75132478-ade8-42e6-9817-e013c36dc0ad",
   "metadata": {},
   "source": [
    "![Clusters for residual magnetic field vs. apparent thorium values](figures_for_jn/f9a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffbaea-af17-4d3f-bd32-764825e81347",
   "metadata": {},
   "source": [
    "**Figure 9a**: Clusters for residual magnetic field vs. apparent thorium values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8aec86-f35b-40dd-90ca-b17eb2be5bc2",
   "metadata": {},
   "source": [
    "![Clusters for residual magnetic field vs. apparent potassium values](figures_for_jn/f9b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15f88f-9569-4231-8463-8195417c3846",
   "metadata": {},
   "source": [
    "**Figure 9b**: Clusters for residual magnetic field vs. apparent thorium values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3e966-633c-45d9-a00d-fb1261c43929",
   "metadata": {},
   "source": [
    "Clusters were then plotted in QGIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0997763-5b95-46ab-95c7-9704d451cc5f",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing clusters generated with K-Means clustering based on residual magnetic field, apparent thorium and apparent potassium values](figures_for_jn/f10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02183e76-fc5b-46d3-af8e-4f465a0136fd",
   "metadata": {},
   "source": [
    "**Figure 10**: A QGIS-generated map showing clusters generated with K-Means clustering based on residual magnetic field, apparent thorium and apparent potassium values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fde3b-e236-4823-a97f-c6f5b09fe915",
   "metadata": {},
   "source": [
    "Interestingly, the Cu-heavy zone corresponds to cluster 1 (blue circles), which aligns with residual magnetic field patterns. Similar patterns were identified in other lines that weren’t as obvious using magnetic data alone. For example, additional faulting was highlighted to the northwest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fe20d-71d9-4d5b-8ed2-7fd2d7ace440",
   "metadata": {},
   "source": [
    "## Anomaly Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499e0b5-1749-4796-8fa7-df7af501fb0a",
   "metadata": {},
   "source": [
    "I decided to try HDBSCAN for anomaly detection, since it was suggested as a potential clustering method in the project instructions. HDBSCAN produces “soft” clusters and adjusts to varying densities, which seemed useful here. I used flight line numbers, longitude, latitude, and residual magnetic fields as inputs. However, most points were flagged as anomalies. I tried tweaking the `min_cluster_size` and `min_samples` parameters but didn’t have much success. The resulting dataset didn’t add much value for mapping potential mineral zones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d3dc8-0c9f-427c-b626-6fdc014b8d80",
   "metadata": {},
   "source": [
    "**anomaly_detection_hdbscan.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03a7f697-e994-4505-81ef-dd1ec6aaf5e6",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data(fp_1, fp_2):\n",
    "    df1 = pd.read_csv(fp_1)  \n",
    "    df2 = pd.read_csv(fp_2)  \n",
    "    \n",
    "    # Concatenate datasets into one larger DataFrame\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Prepare the data to include spatial information (Longitude, Latitude) and line index\n",
    "    X = df[['Flight line number', 'Longitude', 'Latitude', 'Residual magnetic field (comprehensive model CM4)']]\n",
    "\n",
    "    # Normalize the magnetic values only\n",
    "    scaler = StandardScaler()\n",
    "    X.loc[:, 'Residual magnetic field (comprehensive model CM4)'] = scaler.fit_transform(X[['Residual magnetic field (comprehensive model CM4)']])\n",
    "    \n",
    "    return df, X\n",
    "\n",
    "def perform_hdbscan(df, X):\n",
    "    # Perform HDBSCAN clustering\n",
    "    hdb = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=2) # Tweak these values\n",
    "    df['Cluster'] = hdb.fit_predict(X)\n",
    "\n",
    "    # Detect anomalies: points classified as noise (-1)\n",
    "    anomalies = df[df['Cluster'] == -1]\n",
    "    print(f\"Number of detected anomalies: {len(anomalies)}\")\n",
    "    print(anomalies)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "def save_anomalies(anomalies):\n",
    "    output_file_path = \"../data/processed/vector_data/EPSG_4326/reno_walker_lake_combined_mag_anomalies_hdbscan.csv\"\n",
    "    anomalies.to_csv(output_file_path, index=False)\n",
    "    print(f\"Anomalies saved to {output_file_path}\")\n",
    "\n",
    "fp_1 = '../data/processed/vector_data/EPSG_4326/reno_mag.csv'\n",
    "fp_2 = '../data/processed/vector_data/EPSG_4326/walker_lake_mag.csv'\n",
    "df, X = prepare_data(fp_1, fp_2)\n",
    "anomalies = perform_hdbscan(df, X)\n",
    "save_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3aaab-3240-4a79-a711-ed58709fd9ac",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing anomalies detected using the HDBSCAN method with flight line number, longitude, latitude, and residual magnetic field data used as inputs](figures_for_jn/f11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b9bca-e27f-4883-a391-afc24e2e90cd",
   "metadata": {},
   "source": [
    "**Figure 11**: A QGIS-generated map showing anomalies detected using the HDBSCAN method with flight line number, longitude, latitude, and residual magnetic field data used as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3b4d2-1af6-45ae-bc6b-e0cc70d14ae4",
   "metadata": {},
   "source": [
    "For fun, I gave an alternate method, DBSCAN, a shot. Like K-Means, DBSCAN creates hard clusters, but it uses `eps` (the neighborhood distance) instead of `min_cluster_size`. You can estimate `eps` using a k-distance graph, which is similar to the Elbow Method used in K-Means. DBSCAN works best when data points have a uniform density, which fits the reno and walker_lake datasets well. The anomaly detection here produced more useful results, which matched the K-Means clusters. Below is the DBSCAN script, k-distance graph, and anomaly map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693b8e7-fc0f-4e1a-a079-231411703ac3",
   "metadata": {},
   "source": [
    "**anomaly_detection_dbscan.py**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8296312b-234b-4c3f-8052-52208815ccda",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data(fp_1, fp_2):\n",
    "    df1 = pd.read_csv(fp_1)  \n",
    "    df2 = pd.read_csv(fp_2)  \n",
    "    \n",
    "    # Concatenate datasets into one larger DataFrame\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Prepare the data to include spatial information (Longitude, Latitude) and line index\n",
    "    X = df[['Flight line number', 'Longitude', 'Latitude', 'Residual magnetic field (comprehensive model CM4)']]\n",
    "\n",
    "    # Normalize the magnetic values only (optional but recommended for DBSCAN)\n",
    "    scaler = StandardScaler()\n",
    "    X.loc[:, 'Residual magnetic field (comprehensive model CM4)'] = scaler.fit_transform(X[['Residual magnetic field (comprehensive model CM4)']])\n",
    "    \n",
    "    return df, X\n",
    "\n",
    "def calculate_k_distance(X):\n",
    "    # Normalize the magnetic values only\n",
    "    scaler = StandardScaler()\n",
    "    X['Residual Magnetic Field (nT)'] = scaler.fit_transform(X[['Residual magnetic field (comprehensive model CM4)']])\n",
    "\n",
    "    k = 5 \n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    nbrs = neigh.fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "\n",
    "    distances = np.sort(distances[:, k-1], axis=0)  # Sort the k-th nearest neighbor distances\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.ylabel(f'{k}-th Nearest Neighbor Distance')\n",
    "    plt.title('k-distance Graph for Estimating eps')\n",
    "    plt.show()\n",
    "\n",
    "def perform_dbscan(df, X):\n",
    "    dbscan = DBSCAN(eps=0.18, min_samples=8)  # k estimated from k-distance graph; min_samples can be tweaked\n",
    "    df['Cluster'] = dbscan.fit_predict(X)\n",
    "\n",
    "\n",
    "    anomalies = df[df['Cluster'] == -1]\n",
    "    print(f\"Number of detected anomalies: {len(anomalies)}\")\n",
    "    print(anomalies)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "def save_anomalies(anomalies):\n",
    "    output_file_path = \"../data/processed/vector_data/EPSG_4326/reno_walker_lake_combined_mag_anomalies_dbscan.csv\"\n",
    "    anomalies.to_csv(output_file_path, index=False)\n",
    "    print(f\"Anomalies saved to {output_file_path}\")\n",
    "\n",
    "fp_1 = '../data/processed/vector_data/EPSG_4326/reno_mag.csv'\n",
    "fp_2 = '../data/processed/vector_data/EPSG_4326/walker_lake_mag.csv'\n",
    "df, X = prepare_data(fp_1, fp_2)\n",
    "calculate_k_distance(X)\n",
    "anomalies = perform_dbscan(df, X)\n",
    "save_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef74913-08c3-43c1-9082-cef495443618",
   "metadata": {},
   "source": [
    "![The k-distance graph used to estimate `eps`](figures_for_jn/f12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc83fa-a9e1-4e73-a87b-9a9753b79e1c",
   "metadata": {},
   "source": [
    "**Figure 12**: The k-distance graph used to estimate `eps`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c7bd8-eb6b-4180-b345-45bcc1b82744",
   "metadata": {},
   "source": [
    "![A QGIS-generated map showing anomalies detected using the DBSCAN method with flight line number, longitude, latitude, and residual magnetic field data used as inputs](figures_for_jn/f13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73957713-ce14-4d53-a178-714831a98ee9",
   "metadata": {},
   "source": [
    "**Figure 13**: A QGIS-generated map showing anomalies detected using the DBSCAN method with flight line number, longitude, latitude, and residual magnetic field data used as inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
